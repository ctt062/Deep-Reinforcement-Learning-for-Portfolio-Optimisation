\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Findings}

This project investigated the application of Deep Reinforcement Learning to portfolio optimization with integrated risk management mechanisms. Our key findings are:

\begin{enumerate}
    \item \textbf{Comparable Algorithm Performance}: With unified hyperparameters (learning rate: $5\times10^{-5}$, batch size: 128, risk penalty $\lambda=5.0$), both DDPG and PPO achieved comparable risk-adjusted returns (Sharpe: 1.78 vs 1.84). This suggests algorithm choice is less critical than proper risk management design.
    
    \item \textbf{Effective Drawdown Control}: Both agents achieved the $<$10\% maximum drawdown target (DDPG: 9.02\%, PPO: 9.05\%) during the COVID-19 market crash, compared to the market's 33.9\% decline.
    
    \item \textbf{Volatility Targeting Effectiveness}: Scaling exposure by inverse volatility helped both agents maintain consistent $\sim$11\% annualized volatility across varying market conditions.
    
    \item \textbf{Progressive Position Reduction}: The gradual deleveraging mechanism (3\% to 9\% drawdown) preserved capital while maintaining recovery participation.
    
    \item \textbf{Risk Penalty Importance}: High risk penalty ($\lambda=5.0$) proved essential for achieving drawdown targets, demonstrating the importance of reward function design.
\end{enumerate}

\subsection{Contributions}

This work makes the following contributions to algorithmic portfolio management:

\begin{enumerate}
    \item \textbf{Risk Management Framework}: We developed a portfolio optimization framework combining volatility targeting, progressive position reduction, and aggressive drawdown penalties.
    
    \item \textbf{Fair Algorithm Comparison}: We provided rigorous comparison of DDPG and PPO under unified hyperparameters, demonstrating that proper risk management is more important than algorithm selection.
    
    \item \textbf{Stress Test Validation}: We validated the framework during the COVID-19 market crash, achieving $<$10\% drawdown while the market declined 33.9\%.
    
    \item \textbf{Open-Source Implementation}: We provide a complete, modular codebase at \url{https://github.com/ctt062/Deep-Reinforcement-Learning-for-Portfolio-Optimisation}.
\end{enumerate}

\subsection{Limitations}

Several limitations should be considered:

\begin{itemize}
    \item \textbf{Single Test Period}: Results are specific to 2019-2020; performance in other market regimes may differ.
    
    \item \textbf{Transaction Costs}: Our simplified transaction cost model may underestimate real-world implementation costs.
    
    \item \textbf{Market Impact}: We assume no market impact from trading, which may not hold for large portfolios.
    
    \item \textbf{Options Model}: Black-Scholes assumptions may not hold during extreme market conditions.
\end{itemize}

\subsection{Future Work}

Several directions for future research emerge from this work:

\subsubsection{Algorithm Enhancements}

\begin{itemize}
    \item \textbf{Ensemble Methods}: Combining multiple DRL agents could improve robustness and reduce overfitting to specific market regimes.
    
    \item \textbf{Transformer Architectures}: Attention-based models may better capture long-range dependencies in financial time series.
    
    \item \textbf{Meta-Learning}: Training agents that can quickly adapt to new market regimes could improve out-of-sample performance.
\end{itemize}

\subsubsection{Risk Management Extensions}

\begin{itemize}
    \item \textbf{Multi-Asset Options}: Extending hedging to include options on individual assets rather than just the portfolio index.
    
    \item \textbf{Tail Risk Measures}: Incorporating CVaR or Expected Shortfall into the reward function for better tail risk management.
    
    \item \textbf{Regime Detection}: Integrating regime detection models to adapt strategies to different market conditions.
\end{itemize}

\subsubsection{Practical Extensions}

\begin{itemize}
    \item \textbf{Real-Time Trading}: Developing infrastructure for live trading with DRL agents.
    
    \item \textbf{Multi-Asset Classes}: Extending to additional asset classes including futures, currencies, and cryptocurrencies.
    
    \item \textbf{Interpretability}: Developing methods to explain DRL agent decisions for regulatory compliance and risk management.
\end{itemize}

\subsection{Final Remarks}

Deep Reinforcement Learning offers a powerful paradigm for portfolio optimization that can adapt to complex market dynamics. Our results demonstrate that both DDPG and PPO, when combined with proper risk management mechanisms (volatility targeting, progressive position reduction, and aggressive drawdown penalties), can achieve robust risk-adjusted returns and meaningful downside protection during tail risk events.

The key insight from this work is that explicit risk management design is more important than algorithm selection. Both off-policy (DDPG) and on-policy (PPO) approaches achieve similar results when properly configured with unified hyperparameters and consistent risk controls.

The framework developed in this project provides a foundation for practical applications in algorithmic portfolio management, demonstrating that DRL-based approaches can meet institutional-grade risk requirements ($<$10\% max drawdown) while generating positive risk-adjusted returns.
