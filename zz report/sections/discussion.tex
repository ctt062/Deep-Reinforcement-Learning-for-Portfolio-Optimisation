\section{Discussion}
\label{sec:discussion}

\subsection{Algorithm Comparison Under Unified Configuration}

With unified hyperparameters (learning rate: $5\times10^{-5}$, batch size: 128, risk penalty $\lambda=5.0$), both DDPG and PPO achieve comparable risk-adjusted performance. This finding has important implications.

\subsubsection{Off-Policy vs On-Policy Learning}

DDPG (off-policy) and PPO (on-policy) represent fundamentally different learning paradigms:

\begin{itemize}
    \item \textbf{DDPG Advantages}: Sample efficiency through experience replay, stable deterministic policy, direct Q-value optimization.
    
    \item \textbf{PPO Advantages}: Stable policy updates through clipping, better exploration via stochastic policy, simpler hyperparameter tuning.
\end{itemize}

When properly configured, both approaches achieve similar performance, suggesting that the choice of algorithm is less important than proper hyperparameter tuning and risk management design.

\subsubsection{Deterministic vs Stochastic Policies}

DDPG uses a deterministic policy $\ba_t = \mu_\theta(s_t)$, while PPO samples from a distribution $a \sim \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))$.

In our final configuration (reduced entropy coefficient of 0.01), PPO's stochastic policy provides:
\begin{itemize}
    \item Better exploration of the action space during training
    \item Slight edge in risk-adjusted returns (Sharpe: 1.84 vs 1.78)
    \item Similar drawdown control through unified risk penalty
\end{itemize}

\subsection{Importance of Risk Management Design}

The key finding is that explicit risk management mechanisms are more important than algorithm selection:

\subsubsection{Volatility Targeting}
Scaling exposure by inverse volatility ensures consistent risk regardless of market conditions:
\begin{equation}
\text{Exposure} = \min\left(1.0, \frac{\sigma_{\text{target}}}{\sigma_{\text{realized}}}\right)
\end{equation}

\subsubsection{Aggressive Drawdown Penalties}
The reward function with aggressive drawdown penalties ($\lambda=5.0$) incentivizes both agents to maintain $<$10\% max drawdown:
\begin{itemize}
    \item Penalty starts at 2\% drawdown
    \item Exponential scaling as drawdown increases
    \item Massive penalty above 8\% drawdown
\end{itemize}

\subsubsection{Progressive Position Reduction}
Automatic deleveraging starting at 3\% drawdown ensures capital preservation independent of agent actions.

\subsection{Options Hedging Insights}

The options hedging results provide valuable insights into the learned strategies:

\subsubsection{DDPG's Hedging Strategy}

DDPG learned to:
\begin{enumerate}
    \item \textbf{Anticipate Volatility}: Increase hedge ratios before volatility spikes, suggesting learned patterns in market behavior
    
    \item \textbf{Cost-Benefit Analysis}: Maintain hedges only when the expected protection value exceeds premium costs
    
    \item \textbf{Dynamic Adjustment}: Vary hedge ratios based on portfolio composition and market conditions
\end{enumerate}

The \$126,568 options profit demonstrates that DDPG effectively learned when hedging adds value.

\subsubsection{PPO's Conservative Approach}

PPO's lower hedge utilization (23 days vs. 89 days) suggests:
\begin{itemize}
    \item Less confidence in timing hedging decisions
    \item Preference for lower-cost strategies (minimal hedging)
    \item Possible underfitting to the hedging component of the action space
\end{itemize}

\subsection{Risk Management Effectiveness}

Both agents achieved the $<$10\% max drawdown target through the combined effect of:

\begin{enumerate}
    \item \textbf{Volatility Targeting}: Scaled exposure when realized volatility exceeded 10\% target
    \item \textbf{Progressive Position Reduction}: Deleveraging from 3\% to 9\% drawdown
    \item \textbf{Aggressive Reward Penalties}: Risk penalty $\lambda=5.0$ strongly discouraged drawdowns
\end{enumerate}

\begin{table}[htbp]
\centering
\caption{Risk management effectiveness}
\label{tab:risk_effectiveness}
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{Metric} & \textbf{DDPG} & \textbf{PPO} \\
\midrule
Max Drawdown & 9.02\% & 9.05\% \\
Volatility & 10.96\% & 11.09\% \\
VaR (95\%) & 1.08\% & 1.07\% \\
\bottomrule
\end{tabular}
\end{table}

Both agents learned to:
\begin{itemize}
    \item Maintain consistent risk profiles through volatility targeting
    \item Reduce exposure proactively as drawdowns approach limits
    \item Balance return generation with risk control
\end{itemize}

\subsection{Limitations and Considerations}

\subsubsection{Data Limitations}

\begin{itemize}
    \item \textbf{Single Test Period}: Results are from one test period (2019-2020); performance may vary in other market regimes
    
    \item \textbf{Survivorship Bias}: Asset selection based on current knowledge may introduce bias
    
    \item \textbf{Transaction Costs}: Simplified transaction cost model may underestimate real-world costs
\end{itemize}

\subsubsection{Model Limitations}

\begin{itemize}
    \item \textbf{Hyperparameter Sensitivity}: Results depend on hyperparameter choices; extensive tuning on test data could lead to overfitting
    
    \item \textbf{Market Impact}: Models assume no market impact from trading, which may not hold for large portfolios
    
    \item \textbf{Partial Observability}: The state representation may not capture all relevant market information
\end{itemize}

\subsubsection{Options Model Limitations}

\begin{itemize}
    \item \textbf{Black-Scholes Assumptions}: The pricing model assumes constant volatility and log-normal returns
    
    \item \textbf{Execution Assumptions}: Perfect execution at theoretical prices may not be achievable in practice
    
    \item \textbf{Liquidity}: Options on some portfolio constituents may have limited liquidity
\end{itemize}

\subsection{Practical Implications}

For practitioners considering DRL-based portfolio management:

\subsubsection{Algorithm Selection}

\begin{itemize}
    \item \textbf{DDPG preferred} for continuous allocation tasks with stable environments
    \item \textbf{PPO may be preferred} when policy stability is paramount or in more volatile environments requiring frequent adaptation
\end{itemize}

\subsubsection{Risk Management}

\begin{itemize}
    \item Options hedging adds significant value during tail risk events
    \item Tiered stop-loss provides systematic downside protection
    \item Combining multiple risk management tools is more effective than relying on any single approach
\end{itemize}

\subsubsection{Implementation Considerations}

\begin{itemize}
    \item Extensive backtesting across multiple market regimes is essential
    \item Real-time monitoring and human oversight remain important
    \item Regular model retraining may be necessary as market dynamics evolve
\end{itemize}

\subsection{Comparison with Literature}

Our results are consistent with findings in the literature:

\begin{itemize}
    \item \textbf{Jiang et al. (2017)}: Reported similar advantages of deep learning approaches over traditional methods
    
    \item \textbf{Liang et al. (2018)}: Found DDPG effective for portfolio optimization on Chinese markets
    
    \item \textbf{Yang et al. (2020)}: FinRL framework shows comparable performance characteristics
\end{itemize}

However, our contribution extends the literature by:
\begin{enumerate}
    \item Integrating options-based hedging within the DRL framework
    \item Evaluating performance during a specific tail risk event (COVID-19)
    \item Providing detailed comparison between DDPG and PPO for portfolio optimization
\end{enumerate}
