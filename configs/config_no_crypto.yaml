# Configuration for Deep Reinforcement Learning Portfolio Optimization
# Extended test period: 50/50 split (2015-2020 train, 2020-2024 test)
# NO CRYPTO ASSETS - Traditional stocks, ETFs, and commodities only

# Data Configuration
data:
  assets:
    - AAPL      # Tech
    - NVDA      # Tech
    - TSLA      # Tech/Auto
    - MSFT      # Tech
    - GOOGL     # Tech
    - AMZN      # Tech/Consumer
    - SPY       # S&P 500 ETF
    - GLD       # Gold ETF
  start_date: "2015-01-01"
  end_date: "2024-12-31"
  train_ratio: 0.5  # 50/50 split for extended test period
  frequency: "1d"  # 1d for daily, 1wk for weekly
  
# Feature Engineering
features:
  lookback_window: 20  # K: number of historical periods for state
  use_prices: true
  use_returns: true
  use_log_returns: true
  
  # Technical Indicators
  sma_periods: [5, 10, 20]
  ema_periods: [5, 10, 20]
  ema_alpha: 0.1
  momentum_periods: [5, 10, 20]
  
  # Normalization
  normalize_prices: true
  normalize_method: "zscore"  # zscore, minmax, or none
  rolling_normalize: true
  rolling_window: 60

# Environment Configuration
environment:
  initial_balance: 100000.0
  transaction_cost: 0.001  # 0.1% per trade
  allow_short: false  # Long-only constraint
  max_leverage: 1.0
  slippage: 0.0
  
  # Reward Configuration
  reward_type: "risk_adjusted"  # sharpe, risk_adjusted, or log_return
  risk_penalty_lambda: 0.5
  volatility_window: 20
  risk_free_rate: 0.02  # Annual risk-free rate
  
  # Turnover Penalty
  turnover_penalty: 0.0  # Additional penalty for high turnover

# Options Configuration (for options-enhanced models)
options:
  enable_options: true
  option_expiry_days: 30
  option_transaction_cost: 0.005
  max_hedge_ratio: 1.0
  max_call_ratio: 1.0
  put_moneyness: 0.95  # 5% OTM put
  call_moneyness: 1.05  # 5% OTM call
  volatility_lookback: 20
  
# Agent Configuration
agents:
  # Deep Q-Network (DQN)
  dqn:
    policy: "MlpPolicy"
    learning_rate: 0.0001
    buffer_size: 200000
    learning_starts: 5000
    batch_size: 128
    tau: 0.005
    gamma: 0.995
    exploration_fraction: 0.2
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.01
    target_update_interval: 2000
    network_arch: [512, 512, 256]
    n_discrete_actions: 100  # Number of discrete portfolio actions
    
  # Proximal Policy Optimization (PPO)
  ppo:
    policy: "MlpPolicy"
    learning_rate: 0.0001
    n_steps: 4096
    batch_size: 128
    n_epochs: 20
    gamma: 0.995
    gae_lambda: 0.98
    clip_range: 0.15
    ent_coef: 0.0001
    vf_coef: 0.5
    max_grad_norm: 0.5
    network_arch: [512, 512, 256]
    
  # Deep Deterministic Policy Gradient (DDPG)
  ddpg:
    policy: "MlpPolicy"
    learning_rate: 0.0001
    buffer_size: 1000000
    learning_starts: 10000
    batch_size: 256
    tau: 0.005
    gamma: 0.995
    action_noise_std: 0.1
    network_arch:
      pi: [512, 512, 256]
      qf: [512, 512, 256]

# Training Configuration
training:
  total_timesteps: 300000
  eval_freq: 5000
  n_eval_episodes: 10
  save_freq: 10000
  log_interval: 10
  verbose: 1
  seed: 42

# Evaluation Configuration
evaluation:
  n_eval_episodes: 1
  deterministic: true
  render: false
  
# Benchmark Configuration
benchmarks:
  equal_weight: true
  mean_variance: true
  momentum: true
  
  # Mean-Variance Optimization
  mv_lookback: 60
  mv_target_return: null  # null for max Sharpe, float for target return
  mv_allow_short: false
  
  # Momentum Strategy
  momentum_lookback: 20
  momentum_top_k: 3  # Invest in top K assets

# Metrics Configuration
metrics:
  annualized_factor: 252  # 252 for daily, 52 for weekly
  risk_free_rate: 0.02
  confidence_level: 0.95  # For Value at Risk

# Visualization Configuration
visualization:
  figsize: [12, 6]
  dpi: 100
  style: "seaborn-v0_8-darkgrid"
  save_format: "png"
  
# Paths
paths:
  data_dir: "data"
  models_dir: "models_no_crypto"
  results_dir: "results_no_crypto"
  logs_dir: "logs"

# Random Seed for Reproducibility
seed: 42
