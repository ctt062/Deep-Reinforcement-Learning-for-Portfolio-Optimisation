# Configuration for Deep Reinforcement Learning Portfolio Optimization

# Data Configuration
data:
  assets:
    - AAPL
    - NVDA
    - TSLA
    - MSFT
    - GOOGL
    - AMZN
    - SPY
    - GLD
    - BTC-USD
    - ETH-USD
  start_date: "2015-01-01"
  end_date: "2024-12-31"
  train_ratio: 0.7
  frequency: "1d"  # 1d for daily, 1wk for weekly
  
# Feature Engineering
features:
  lookback_window: 20  # K: number of historical periods for state
  use_prices: true
  use_returns: true
  use_log_returns: true
  
  # Technical Indicators
  sma_periods: [5, 10, 20]
  ema_periods: [5, 10, 20]
  ema_alpha: 0.1
  momentum_periods: [5, 10, 20]
  
  # Normalization
  normalize_prices: true
  normalize_method: "zscore"  # zscore, minmax, or none
  rolling_normalize: true
  rolling_window: 60

# Environment Configuration
environment:
  initial_balance: 100000.0
  transaction_cost: 0.001  # 0.1% per trade
  allow_short: false  # Long-only constraint
  max_leverage: 1.0
  slippage: 0.0
  
  # Reward Configuration
  reward_type: "risk_adjusted"  # sharpe, risk_adjusted, or log_return
  risk_penalty_lambda: 0.5
  volatility_window: 20
  risk_free_rate: 0.02  # Annual risk-free rate
  
  # Turnover Penalty
  turnover_penalty: 0.0  # Additional penalty for high turnover
  
# Agent Configuration
agents:
  # Deep Q-Network (DQN)
  dqn:
    policy: "MlpPolicy"
    learning_rate: 0.0001
    buffer_size: 100000
    learning_starts: 1000
    batch_size: 32
    tau: 1.0
    gamma: 0.99
    exploration_fraction: 0.1
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.05
    target_update_interval: 1000
    network_arch: [128, 128]
    
  # Proximal Policy Optimization (PPO)
  ppo:
    policy: "MlpPolicy"
    learning_rate: 0.0001
    n_steps: 4096
    batch_size: 128
    n_epochs: 20
    gamma: 0.995
    gae_lambda: 0.98
    clip_range: 0.15
    ent_coef: 0.0001
    vf_coef: 0.5
    max_grad_norm: 0.5
    network_arch: [512, 512, 256]
    
  # Deep Deterministic Policy Gradient (DDPG)
  ddpg:
    policy: "MlpPolicy"
    learning_rate: 0.001
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 100
    tau: 0.005
    gamma: 0.99
    action_noise: "normal"  # normal or ornstein_uhlenbeck
    noise_std: 0.1
    network_arch:
      pi: [128, 128]
      qf: [128, 128]

# Training Configuration
training:
  total_timesteps: 300000
  eval_freq: 5000
  n_eval_episodes: 10
  save_freq: 10000
  log_interval: 10
  verbose: 1
  seed: 42

# Evaluation Configuration
evaluation:
  n_eval_episodes: 1
  deterministic: true
  render: false
  
# Benchmark Configuration
benchmarks:
  equal_weight: true
  mean_variance: true
  momentum: true
  
  # Mean-Variance Optimization
  mv_lookback: 60
  mv_target_return: null  # null for max Sharpe, float for target return
  mv_allow_short: false
  
  # Momentum Strategy
  momentum_lookback: 20
  momentum_top_k: 3  # Invest in top K assets

# Metrics Configuration
metrics:
  annualized_factor: 252  # 252 for daily, 52 for weekly
  risk_free_rate: 0.02
  confidence_level: 0.95  # For Value at Risk

# Visualization Configuration
visualization:
  figsize: [12, 6]
  dpi: 100
  style: "seaborn-v0_8-darkgrid"
  save_format: "png"
  
# Paths
paths:
  data_dir: "data"
  models_dir: "models"
  results_dir: "results"
  logs_dir: "logs"

# Random Seed for Reproducibility
seed: 42
