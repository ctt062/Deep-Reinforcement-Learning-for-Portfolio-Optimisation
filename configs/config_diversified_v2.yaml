# Configuration for Deep Reinforcement Learning Portfolio Optimization
# Diversified Portfolio - Version 2 (Enhanced for Lower Drawdown)
# Focus: Industry-standard diversification with aggressive risk management

# Data Configuration
data:
  # Well-diversified portfolio across sectors and asset classes (NO CRYPTO)
  assets:
    # Technology (15%)
    - AAPL      # Apple - Consumer Electronics
    - MSFT      # Microsoft - Software
    - GOOGL     # Alphabet - Internet Services
    
    # Healthcare (15%)
    - JNJ       # Johnson & Johnson - Pharmaceuticals
    - UNH       # UnitedHealth - Health Insurance
    - PFE       # Pfizer - Pharmaceuticals
    
    # Financials (10%)
    - JPM       # JPMorgan Chase - Banking
    - V         # Visa - Payment Processing
    
    # Consumer (10%)
    - AMZN      # Amazon - E-commerce
    - WMT       # Walmart - Retail
    - PG        # Procter & Gamble - Consumer Goods
    
    # Energy (5%)
    - XOM       # ExxonMobil - Oil & Gas
    
    # Industrials (5%)
    - BA        # Boeing - Aerospace
    - CAT       # Caterpillar - Industrial Equipment
    
    # Broad Market ETFs (20%)
    - SPY       # S&P 500 ETF
    - QQQ       # NASDAQ 100 ETF
    - IWM       # Russell 2000 Small Cap ETF
    
    # Fixed Income / Bonds (15%)
    - TLT       # 20+ Year Treasury Bonds
    - IEF       # 7-10 Year Treasury Bonds
    - AGG       # Aggregate Bond Index
    
    # Commodities (5%)
    - GLD       # Gold ETF
    - SLV       # Silver ETF
    
  start_date: "2010-01-01"
  end_date: "2024-12-31"
  train_ratio: 0.70  # 70/30 split
  frequency: "1d"
  
# Feature Engineering
features:
  lookback_window: 20
  use_prices: true
  use_returns: true
  use_log_returns: true
  
  # Technical Indicators
  sma_periods: [5, 10, 20, 50]
  ema_periods: [5, 10, 20]
  ema_alpha: 0.1
  momentum_periods: [5, 10, 20]
  
  # Normalization
  normalize_prices: true
  normalize_method: "zscore"
  rolling_normalize: true
  rolling_window: 60

# Environment Configuration - ENHANCED FOR LOWER DRAWDOWN
environment:
  initial_balance: 100000.0
  transaction_cost: 0.001
  allow_short: false
  max_leverage: 1.0
  slippage: 0.0
  
  # Reward Configuration - AGGRESSIVE RISK PENALTIES
  reward_type: "risk_adjusted"
  risk_penalty_lambda: 2.0  # Increased from 0.5 to 2.0 for aggressive risk management
  volatility_window: 20
  risk_free_rate: 0.02
  
  # Turnover Penalty - encourage lower trading
  turnover_penalty: 0.001  # Increased to discourage excessive trading
  
  # Additional risk constraints
  max_position_size: 0.25  # Max 25% in any single asset
  min_position_size: 0.02  # Min 2% to ensure diversification

# Agent Configuration - OPTIMIZED FOR STABILITY
agents:
  # Deep Q-Network (DQN)
  dqn:
    policy: "MlpPolicy"
    learning_rate: 0.00005  # Reduced for more stable learning
    buffer_size: 500000  # Increased buffer
    learning_starts: 10000
    batch_size: 256  # Increased batch size
    tau: 0.001  # Slower target network updates
    gamma: 0.99  # Slightly reduced for near-term focus
    exploration_fraction: 0.3
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.01
    target_update_interval: 5000
    network_arch: [512, 512, 256, 256]  # Deeper network
    n_discrete_actions: 100
    
  # Proximal Policy Optimization (PPO)
  ppo:
    policy: "MlpPolicy"
    learning_rate: 0.00005  # Reduced for stability
    n_steps: 4096
    batch_size: 256  # Increased
    n_epochs: 20
    gamma: 0.99
    gae_lambda: 0.98
    clip_range: 0.15
    ent_coef: 0.001  # Increased entropy for exploration
    vf_coef: 0.5
    max_grad_norm: 0.5
    network_arch: [512, 512, 256, 256]  # Deeper network
    
  # Deep Deterministic Policy Gradient (DDPG)
  ddpg:
    policy: "MlpPolicy"
    learning_rate: 0.00005  # Reduced
    buffer_size: 1000000
    learning_starts: 20000  # Increased warm-up
    batch_size: 512  # Increased batch size
    tau: 0.001  # Slower updates
    gamma: 0.99
    action_noise_std: 0.05  # Reduced noise
    network_arch:
      pi: [512, 512, 256, 256]
      qf: [512, 512, 256, 256]

# Training Configuration
training:
  total_timesteps: 500000  # Increased training time
  eval_freq: 10000
  n_eval_episodes: 10
  save_freq: 10000
  log_interval: 10
  verbose: 1
  seed: 42

# Evaluation Configuration
evaluation:
  n_eval_episodes: 1
  deterministic: true
  render: false
  
# Benchmark Configuration
benchmarks:
  equal_weight: true
  mean_variance: true
  momentum: true
  
  mv_lookback: 60
  mv_target_return: null
  mv_allow_short: false
  
  momentum_lookback: 20
  momentum_top_k: 5  # Top 5 assets

# Metrics Configuration
metrics:
  annualized_factor: 252
  risk_free_rate: 0.02
  confidence_level: 0.95

# Visualization Configuration
visualization:
  figsize: [14, 8]
  dpi: 100
  style: "seaborn-v0_8-darkgrid"
  save_format: "png"
  
# Paths
paths:
  data_dir: "data"
  models_dir: "models_diversified_v2"
  results_dir: "results_diversified_v2"
  logs_dir: "logs"

# Random Seed
seed: 42
