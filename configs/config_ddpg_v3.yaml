# DDPG V3: Final optimization to push Sharpe from 0.988 to >1.0
# Already has excellent DD control (9.09%), just need slightly more returns

data:
  data_dir: data
  assets:
  - AAPL
  - MSFT
  - GOOGL
  - NVDA
  - AMZN
  - JNJ
  - UNH
  - PFE
  - JPM
  - V
  - WMT
  - COST
  - SPY
  - QQQ
  - IWM
  - TLT
  - AGG
  - GLD
  start_date: '2010-01-01'
  end_date: '2024-12-31'
  train_ratio: 0.7
  frequency: 1d

features:
  lookback_window: 60
  use_prices: true
  use_returns: true
  use_log_returns: true
  sma_periods: [10, 20, 50, 200]
  ema_periods: [10, 20, 50]
  ema_alpha: 0.1
  momentum_periods: [5, 10, 20, 60]
  normalize_prices: true
  normalize_method: zscore
  rolling_normalize: true
  rolling_window: 252

environment:
  initial_balance: 100000.0
  transaction_cost: 0.001
  allow_short: false
  max_leverage: 1.0
  slippage: 0.0
  reward_type: risk_adjusted
  risk_penalty_lambda: 0.75  # REDUCED from 0.8 (V2) to 0.75 for more returns
  volatility_window: 60
  risk_free_rate: 0.02
  turnover_penalty: 0.0003  # REDUCED from 0.0005 to allow even more trading
  min_position_size: 0.0
  max_position_size: 0.27  # INCREASED from 0.25 to 0.27 for more concentration
  enable_stop_loss: true
  stop_loss_threshold: 0.05
  stop_loss_action: defensive
  stop_loss_recovery_threshold: 0.02

options:
  enabled: true
  protective_puts:
    enabled: true
    max_hedge_ratio: 0.25  # Keep at 0.25 (working well in V2)
    strike_pct: 0.96  # Keep at 96% (good balance)
    expiry_days: 30
    cost_factor: 0.015
  covered_calls:
    enabled: true
    max_coverage_ratio: 0.30  # INCREASED from 0.25 to 0.30 for more income
    strike_pct: 1.035  # REDUCED from 1.04 to 1.035 (more aggressive)
    expiry_days: 30
    premium_factor: 0.015

agents:
  ddpg:
    policy: MlpPolicy
    learning_rate: 1.2e-04  # INCREASED from 1.0e-04 for faster learning
    batch_size: 256
    buffer_size: 500000
    learning_starts: 10000
    gamma: 0.99
    tau: 0.01
    action_noise: 0.15
    train_freq: 1
    gradient_steps: 1
    network_arch:
      pi: [512, 512, 256, 128]
      qf: [512, 512, 256, 128]

training:
  total_timesteps: 500000
  eval_freq: 50000
  n_eval_episodes: 5
  save_freq: 50000
  log_interval: 10
  verbose: 1
  tensorboard_log: logs
  device: cpu

evaluation:
  n_eval_episodes: 1
  deterministic: true
  render: false

benchmarks:
  equal_weight: true
  mean_variance: true
  momentum: true
  mv_lookback: 252
  mv_target_return: null
  mv_allow_short: false

metrics:
  annualized_factor: 252
  risk_free_rate: 0.02
  confidence_level: 0.95

visualization:
  figsize: [12, 6]
  dpi: 100
  style: seaborn-v0_8-darkgrid
  save_format: png

paths:
  data_dir: data
  models_dir: models_ddpg_v3
  results_dir: results_ddpg_v3
  logs_dir: logs

seed: 42
