data:
  data_dir: data
  assets:
  - AAPL
  - MSFT
  - GOOGL
  - NVDA
  - META
  - ADBE
  - CRM
  - ORCL
  - AMZN
  - TSLA
  - WMT
  - COST
  - NKE
  - JPM
  - BAC
  - V
  - MA
  - JNJ
  - UNH
  - PFE
  - ABBV
  - XOM
  - CVX
  - SPY
  - QQQ
  - IWM
  - GLD
  - SLV
  - TLT
  - IEF
  - SHY
  - AGG
  - LQD
  start_date: '2010-01-01'
  end_date: '2024-12-31'
  train_ratio: 0.7
  frequency: 1wk

features:
  lookback_window: 26
  use_prices: true
  use_returns: true
  use_log_returns: true
  sma_periods:
  - 4
  - 13
  - 26
  - 52
  ema_periods:
  - 4
  - 13
  - 26
  - 52
  ema_alpha: 0.1
  momentum_periods:
  - 4
  - 13
  - 26
  - 52
  normalize_prices: true
  normalize_method: zscore
  rolling_normalize: true
  rolling_window: 52

environment:
  initial_balance: 100000.0
  transaction_cost: 0.001
  allow_short: false
  max_leverage: 1.0
  slippage: 0.0
  reward_type: risk_adjusted
  risk_penalty_lambda: 1.5  # Reduced from 4.0 - more balanced risk/return trade-off
  volatility_window: 26
  risk_free_rate: 0.02
  turnover_penalty: 0.001  # Added to discourage excessive trading
  # Position sizing constraints
  min_position_size: 0.0  # Allow zero weight (can exclude assets)
  max_position_size: 0.20  # Max 20% per asset for diversification

# Options overlay configuration
options:
  enabled: true
  protective_puts:
    enabled: true
    max_hedge_ratio: 0.5  # Reduced from 1.0 - hedge up to 50% instead of 100%
    strike_pct: 0.95  # 5% OTM puts
    expiry_days: 30
    cost_factor: 0.02  # Reduced option costs
  covered_calls:
    enabled: true
    max_coverage_ratio: 0.3  # Reduced from 1.0 - only cover 30% of holdings
    strike_pct: 1.05  # 5% OTM calls
    expiry_days: 30
    premium_factor: 0.015  # More realistic premium

agents:
  ppo:
    policy: MlpPolicy
    learning_rate: 5.0e-05  # Increased from 2e-05 for faster learning
    n_steps: 4096
    batch_size: 256
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.05  # Increased from 0.03 for more exploration
    vf_coef: 0.5
    max_grad_norm: 0.5
    network_arch:
    - 1024
    - 512
    - 256
    - 128
  ddpg:
    policy: MlpPolicy
    learning_rate: 5.0e-05  # Increased from 2e-05
    batch_size: 512
    buffer_size: 500000
    learning_starts: 20000
    gamma: 0.99
    tau: 0.001
    action_noise: 0.1  # Increased from 0.05 for more exploration
    train_freq: 1
    gradient_steps: 1
    network_arch:
      pi:
      - 1024
      - 512
      - 256
      - 128
      qf:
      - 1024
      - 512
      - 256
      - 128
  dqn:
    policy: MlpPolicy
    learning_rate: 5.0e-05
    batch_size: 256
    buffer_size: 500000
    learning_starts: 10000
    gamma: 0.99
    target_update_interval: 5000
    train_freq: 4
    gradient_steps: 1
    exploration_fraction: 0.3
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.01
    network_arch:
    - 1024
    - 512
    - 256
    - 128

training:
  total_timesteps: 750000
  eval_freq: 75000
  n_eval_episodes: 5
  save_freq: 75000
  log_interval: 10
  verbose: 1
  tensorboard_log: logs
  device: cpu

evaluation:
  n_eval_episodes: 1
  deterministic: true
  render: false

benchmarks:
  equal_weight: true
  mean_variance: true
  momentum: true
  mv_lookback: 52
  mv_target_return: null
  mv_allow_short: false

metrics:
  annualized_factor: 52
  risk_free_rate: 0.02
  confidence_level: 0.95

visualization:
  figsize:
  - 12
  - 6
  dpi: 100
  style: seaborn-v0_8-darkgrid
  save_format: png

paths:
  data_dir: data
  models_dir: models_realistic_v2
  results_dir: results_realistic_v2
  logs_dir: logs

seed: 42
